{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-vllm-metax","title":"Welcome to vLLM-MetaX","text":"<p> MetaX Hardware Backend Plugin  </p>"},{"location":"getting_started/quickstart.html","title":"Quickstart","text":""},{"location":"getting_started/quickstart.html#quickstart","title":"Quickstart","text":"<p>Currently the recommanded way to start vLLM-MetaX is via docker.</p> <p>Belows is version mapping to released plugin and maca:</p> plugin version maca version docker distribution tag v0.8.5 maca2.33.1.13 vllm:maca.ai2.33.1.13-torch2.6-py310-ubuntu22.04-amd64 v0.9.1 maca3.0.0.5 vllm:maca.ai3.0.0.5-torch2.6-py310-ubuntu22.04-amd64 v0.10.1.1 (dev) maca3.1.0.7 not released v0.10.2 maca3.2.0.7 not released v0.11.0 maca3.2.x.x not released master maca3.2.x.x. not released <p>Note: All the vllm tests are based on the related maca version. Using incorresponding version of maca for vllm may cause unexpected bugs or errors. This is not garanteed.</p> <p>vLLM-MetaX is out of box via these docker images.</p>"},{"location":"getting_started/quickstart.html#offline-batched-inference","title":"Offline Batched Inference","text":""},{"location":"getting_started/quickstart.html#openai-compatible-server","title":"OpenAI-Compatible Server","text":""},{"location":"getting_started/quickstart.html#on-attention-backends","title":"On Attention Backends","text":""},{"location":"getting_started/installation/index.html","title":"Installation","text":""},{"location":"getting_started/installation/index.html#installation","title":"Installation","text":"<p>If you want to develop or debug vllm-metax plugin, you may need to build and install the plugin from source.</p> <p>Currently we recommanded you to start from existed docker images on MetaX Develop Community and following the MACA installation guide.</p>"},{"location":"getting_started/installation/MACA.html","title":"MACA","text":""},{"location":"getting_started/installation/MACA.html#maca","title":"MACA","text":""},{"location":"getting_started/installation/MACA.html#requirements","title":"Requirements","text":"<ul> <li>OS: Linux</li> <li>Python: 3.10 -- 3.12</li> </ul>"},{"location":"getting_started/installation/MACA.html#set-up-using-pip-without-uv","title":"Set up using pip (without UV)","text":""},{"location":"getting_started/installation/MACA.html#build-wheel-from-source","title":"Build wheel from source","text":"<p>Note</p> <p>If using pip, all the build and installation steps are based on corresponding docker images. You can find them on quick start. We need to add <code>-no-build-isolation</code> flag (or an equivalent one) during package building, since all the requirements are already pre-installed in released docker image.</p>"},{"location":"getting_started/installation/MACA.html#setup-environment-variables","title":"Setup environment variables","text":"<pre><code># setup MACA path\nexport MACA_PATH=\"/opt/maca\"\n\n# cu-bridge\nexport CUCC_PATH=\"${MACA_PATH}/tools/cu-bridge\"\nexport CUDA_PATH=/root/cu-bridge/CUDA_DIR\nexport CUCC_CMAKE_ENTRY=2\n\n# update PATH\nexport PATH=${MACA_PATH}/mxgpu_llvm/bin:${MACA_PATH}/bin:${CUCC_PATH}/tools:${CUCC_PATH}/bin:${PATH}\nexport LD_LIBRARY_PATH=${MACA_PATH}/lib:${MACA_PATH}/ompi/lib:${MACA_PATH}/mxgpu_llvm/lib:${LD_LIBRARY_PATH}\n\nexport VLLM_INSTALL_PUNICA_KERNELS=1\n</code></pre>"},{"location":"getting_started/installation/MACA.html#build-vllm","title":"Build vllm","text":"<p>Clone vllm project:</p> <pre><code>git clone  --depth 1 --branch main https://github.com/vllm-project/vllm\ncd vllm\n</code></pre> <p>Build with empty device:</p> <pre><code>python use_existing_torch.py\npip install -r requirements/build.txt\nVLLM_TARGET_DEVICE=empty pip install -v . --no-build-isolation\n</code></pre>"},{"location":"getting_started/installation/MACA.html#build-plugin","title":"Build plugin","text":"<p>Install the build requirments first:</p> <pre><code>python use_existing_metax.py\npip install -r requirements/build.txt\n</code></pre> <p>Build and install vLLM:</p> <pre><code>pip install . -v --no-build-isolation\n</code></pre> <p>If you want to develop vLLM, install it in editable mode instead.</p> <pre><code>pip install . -e -v --no-build-isolation\n</code></pre> <p>Optionally, build a portable wheel which you can then install elsewhere:</p> <pre><code>python -m build -w -n\npip install dist/*.whl\n</code></pre>"},{"location":"getting_started/installation/MACA.html#set-up-using-uv-experimental","title":"Set up using UV (experimental)","text":"<p>Todo</p>"},{"location":"getting_started/installation/MACA.html#extra-information","title":"Extra information","text":""}]}